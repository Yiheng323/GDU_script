{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebok that drafts a small script to pull in all the data for the WGS pathogen detection and microbiome data. It needs to pull in the following:\n",
    "    * sequence summary file from albacore\n",
    "    * one blast output files (I used ncbi nt database)\n",
    "    * get the length of all porchoped reads\n",
    "    * get processing (T/F) for porchoped\n",
    "    * get this all into one data frame\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess as sub\n",
    "from Bio import SeqIO\n",
    "import argparse\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='''\n",
    "#This is a notebok that drafts a small script to pull in all the data for the WGS pathogen detection and microbiome data. It needs to pull in the following:\n",
    "\n",
    "#    * sequence summary file from albacore\n",
    "#    * two blast output files (specific database and NCBI)\n",
    "#    * get the length of all porchoped reads\n",
    "#    * get processing (T/F) for porchoped and nanolyze\n",
    "#    * get this all into one data frame\n",
    "#''')\n",
    "\n",
    "#parser.add_argument(\"BASEDIR\", help=\"base folder, supposed to have all the sub folders processed by WGS script. The same as Indir in YH_script2. DO NOT add the back slash'/' at the end!\")\n",
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets define the base folder\n",
    "BASEDIR = '/home/yiheng/data/20170617_FAH05432'\n",
    "#BASEDIR = args.BASEDIR\n",
    "#this will become the only flag of argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a quick check that looks for all the right folders\n",
    "folder_list = 'basecalled_data  scripts  tracking  workspace'.split(' ')\n",
    "for x in range(0,folder_list.count('')):\n",
    "    folder_list.remove('')\n",
    "#fix this test\n",
    "if not set(os.listdir(os.path.abspath(BASEDIR))) >= set (folder_list):\n",
    "    print(\"Something wrong with basefolder. check it please.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the columns that you want to pick up from sequencing summary file.\n",
    "# Here is the columns I chose for plotting out data, enough information for me so I did not pick others.\n",
    "seq_df_headers = ['read_id','passes_filtering', 'sequence_length_template', 'mean_qscore_template',\\\n",
    "                  'barcode_arrangement', 'barcode_score', 'kit', 'variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now getting the headers from /home/yiheng/data/20170617_FAH05432/basecalled_data/Hu_FAH05432_albacore202/sequencing_summary.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yiheng/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (3,4,5,6,7,8,9,10,11,12,13,14,16,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#now get the headers sequencing_summary file\n",
    "base_called_folder = os.path.join(BASEDIR, 'basecalled_data')\n",
    "\n",
    "# here added a function to check the tar.gz file and its corresponding unzipped folder in the basecalled_data folder.  \n",
    "# If it is not unzipped, unzip it.\n",
    "for thing in os.listdir(base_called_folder):\n",
    "    judge_list = [os.path.isdir(os.path.join(base_called_folder, thing))]\n",
    "\n",
    "if any(x == True for x in judge_list):\n",
    "    seq_sum_file = os.path.join(base_called_folder, thing, 'sequencing_summary.txt')\n",
    "    if not os.path.exists(seq_sum_file):\n",
    "        print('No sequencing summary file from basecalled folder. Please go check')\n",
    "        \n",
    "    print(\"now getting the headers from %s\" % seq_sum_file)\n",
    "    seq_df = pd.read_csv(seq_sum_file, sep='\\t')\n",
    "    #capture the thing as the prefix of the fastq/fasta files in the barcode folders\n",
    "    prefix = thing\n",
    "    #might be a better way to only read in the wanted columns. Not subsetting afterwards.\n",
    "    #please go check\n",
    "    seq_df = seq_df.loc[:, seq_df_headers].copy()\n",
    "    \n",
    "elif all(x == False for x in judge_list):\n",
    "        \n",
    "    zipped_basecalled_file = os.path.join(base_called_folder, thing)\n",
    "    if zipped_basecalled_file.endswith('tar.gz'):\n",
    "        print(\"now unzipping file %s.\" % zipped_basecalled_file)\n",
    "        tar = tarfile.open(zipped_basecalled_file)\n",
    "        tar.extractall(base_called_folder.split(\".\")[0])\n",
    "        tar.close()\n",
    "    else:\n",
    "        print(\"there is something strange in the basecalled folder, please check.\")\n",
    "else:\n",
    "        print(\"there is something strange in the basecalled folder, please check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now get all the rgblast_output databases done \n",
    "rg_blast_df_file_list = []\n",
    "nt_blast_df_file_list = []\n",
    "workspace = os.path.join(BASEDIR, 'workspace')\n",
    "folder_counter = 0\n",
    "for folder in os.listdir(workspace):\n",
    "    folder = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    for file in os.listdir(folder):\n",
    "        if not file.endswith('.rgblast_output') or not file.endswith('.ntblast_output'):\n",
    "            next\n",
    "        if file.endswith('.rgblast_output'):\n",
    "            rg_blast_df_file_list.append(os.path.join(folder, file))\n",
    "        if file.endswith('.ntblast_output'):\n",
    "            nt_blast_df_file_list.append(os.path.join(folder, file))\n",
    "    if not len(nt_blast_df_file_list) == len(rg_blast_df_file_list) == folder_counter:\n",
    "        print('Not all barcode folders have all blast output files')\n",
    "    else:\n",
    "        next\n",
    "    #print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode02/Hu_FAH05432_albacore202.barcode02.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode01/Hu_FAH05432_albacore202.barcode01.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode07/Hu_FAH05432_albacore202.barcode07.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode06/Hu_FAH05432_albacore202.barcode06.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode03/Hu_FAH05432_albacore202.barcode03.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode05/Hu_FAH05432_albacore202.barcode05.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode04/Hu_FAH05432_albacore202.barcode04.fastq\n",
      "now processing nanolyzed file /home/yiheng/data/20170617_FAH05432/workspace/barcode00/Hu_FAH05432_albacore202.barcode00.fastq\n"
     ]
    }
   ],
   "source": [
    "#now get all the names of the reads that survived the lyzing\n",
    "workspace = os.path.join(BASEDIR, 'workspace')\n",
    "folder_counter = 0\n",
    "nanolyze_readid_list = []\n",
    "for folder in os.listdir(workspace):\n",
    "    folder_long = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder_long):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    for file in os.listdir(folder_long):\n",
    "        lyzed_fastq = (os.path.join(folder_long, '%s.%s.fastq' % (prefix, folder )))\n",
    "        if not os.path.exists(lyzed_fastq):\n",
    "            print(\"No lyzed fastq file present.\")\n",
    "        elif file == '%s.%s.fastq' % (prefix, folder ):\n",
    "            print(\"now processing nanolyzed file %s\" % lyzed_fastq)\n",
    "            lyzed_fastq_name = '%s.name.tmp' % lyzed_fastq\n",
    "            #cmd = \"grep '^@' %s | cut -c1-37 > %s\"%\\\n",
    "            # (lyzed_fastq, lyzed_fastq_name)\n",
    "            #cmd = \"grep '^@' %s | cut -d:-f1 > %s\"%\\\n",
    "             #(lyzed_fastq, lyzed_fastq_name)\n",
    "            cmd = r\"grep '^@' %s | sed -e 's/@\\([a-zA-Z0-9-]\\+\\).*/\\1/' > %s\"%\\\n",
    "            (lyzed_fastq, lyzed_fastq_name)\n",
    "            #print(cmd)\n",
    "            sub.check_output(cmd, shell=True, stderr=sub.STDOUT)\n",
    "            #sub.run(cmd.split(' '))\n",
    "            nanolyze_readid_list.append(lyzed_fastq_name)\n",
    "        else:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now read in the tmp file of nanolyzed ids and add them to the dataframe as column\n",
    "def get_read_ids(filename_list):\n",
    "    #write a check that both _list and header is a list\n",
    "    read_id_list = []\n",
    "    for filename in filename_list:\n",
    "        tmp_list = pd.read_csv(filename, sep='\\t', header=None).loc[:,0].tolist()\n",
    "        read_id_list= read_id_list + tmp_list\n",
    "    return read_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nl_survied_list = get_read_ids(nanolyze_readid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode02/Hu_FAH05432_albacore202.chopped.barcode02.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode01/Hu_FAH05432_albacore202.chopped.barcode01.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode07/Hu_FAH05432_albacore202.chopped.barcode07.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode06/Hu_FAH05432_albacore202.chopped.barcode06.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode03/Hu_FAH05432_albacore202.chopped.barcode03.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode05/Hu_FAH05432_albacore202.chopped.barcode05.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode04/Hu_FAH05432_albacore202.chopped.barcode04.fastq.\n",
      "now processing porechopped file /home/yiheng/data/20170617_FAH05432/workspace/barcode00/Hu_FAH05432_albacore202.chopped.barcode00.fastq.\n"
     ]
    }
   ],
   "source": [
    "#do same for porechop + length\n",
    "#this is pretty slow. May consider parallzing.\n",
    "pc_length_dict = {}\n",
    "porechop_survived_list = []\n",
    "folder_counter = 0\n",
    "for folder in os.listdir(workspace):\n",
    "    folder_long = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder_long):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    porechoped_file = os.path.join(folder_long,'%s.chopped.%s.fastq'%(prefix, folder))\n",
    "    print(\"now processing porechopped file %s.\" % porechoped_file)\n",
    "    if not os.path.exists(porechoped_file):\n",
    "            print(\"Porechopped fastq missing for %s.\" % folder)\n",
    "    else:\n",
    "        for seq in SeqIO.parse(porechoped_file, 'fastq'):\n",
    "            pc_length_dict[seq.id] = len(seq.seq)\n",
    "            porechop_survived_list.append(seq.id)\n",
    "    \n",
    "#take for loop from above to loop over chopped files\n",
    "#filename = '/home/yiheng/data/Wagga_run1/workspace/barcode01/Wagga_run1_albacore202.chopped.barcode01.fastq'\n",
    "#for seq in SeqIO.parse(filename, 'fastq'):\n",
    "#    pc_length_dict[seq.id] = len(seq.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the porechopped reads will be splited into two reads they will have an _ in their read id\n",
    "porechop_survived_single_list = [x.split('_')[0] for x in porechop_survived_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add porechop survived column\n",
    "seq_df['pc_survived'] = seq_df['read_id'].isin(porechop_survived_single_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add the nanolyze survived column\n",
    "seq_df['nl_survived'] = seq_df['read_id'].isin(nl_survied_list)\n",
    "#make porchoped survived column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blast_header = ['qseqid',\n",
    " 'sseqid',\n",
    " 'evalue',\n",
    " 'bitscore',\n",
    " 'length',\n",
    " 'pident',\n",
    " 'nident',\n",
    " 'sgi',\n",
    " 'sacc',\n",
    " 'staxids',\n",
    " 'scomnames',\n",
    "'sskingdoms']\n",
    "# original headers: qseqid sseqid evalue bitscore length pident nident sgi sacc staxids sscinames scomnames sskingdoms\n",
    "def make_all_blast_df(_list, header, chopped_len_dict):\n",
    "    df = pd.DataFrame()\n",
    "    #write a check that both _list and header is a list\n",
    "    for x in _list:\n",
    "        tmp_df = pd.read_csv(x, sep='\\t',header=None, names=header)\n",
    "        first_column = tmp_df.columns[0]\n",
    "        tmp_df['read_id'] = tmp_df[first_column].apply(lambda x: str(x).split('_')[0])\n",
    "        tmp_df['read_length_pc'] = tmp_df[first_column].apply(lambda x: chopped_len_dict[x])\n",
    "        df = pd.concat([df, tmp_df.iloc[:,[0,1,2,4,5,6,8,9,10,12,13]]])\n",
    "        \n",
    "    #now reduce the columns to what we want\n",
    "    \n",
    "    #print(first_column)\n",
    "    #df['read_id'] = df.iloc[:,0].str.split('_')[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now adding the nt output columns.\n",
      "now adding the rg output columns.\n"
     ]
    }
   ],
   "source": [
    "nt_df = make_all_blast_df(nt_blast_df_file_list, [x +'_nt' for x in  blast_header], pc_length_dict)\n",
    "print(\"now adding the nt output columns.\")\n",
    "rg_df = make_all_blast_df(rg_blast_df_file_list, [x +'_rg' for x in  blast_header], pc_length_dict)\n",
    "print(\"now adding the rg output columns.\")\n",
    "#reduce column number of blast dataframe to what you want before you merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now creating the final dataframe!\n"
     ]
    }
   ],
   "source": [
    "#need to take care of porchop split reads checkin if second last character of string is _\n",
    "#make a new column of the blast_df that has the initial read_id\n",
    "#need to check how merge behaves when getting a doublcate of value in one df.\n",
    "tmp_df = pd.merge(seq_df, rg_df,how='outer',left_on= 'read_id', right_on='read_id')\n",
    "final_df = pd.merge(tmp_df, nt_df,how='outer',left_on= 'read_id', right_on='read_id')\n",
    "print(\"now creating the final dataframe!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Congratulations!\n"
     ]
    }
   ],
   "source": [
    "analysis_foler = os.path.join(BASEDIR, 'analysis')\n",
    "if not os.path.exists(analysis_foler):\n",
    "    os.mkdir(analysis_foler)\n",
    "final_df_fn = os.path.join(analysis_foler, 'summary_df_%s.tab' % (os.path.basename(BASEDIR)))\n",
    "final_df.to_csv(final_df_fn, sep='\\t', index=None)\n",
    "print(\"All done. Congratulations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
