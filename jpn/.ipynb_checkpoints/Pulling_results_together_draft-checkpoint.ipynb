{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebok that drafts a small script to pull in all the data for the WGS pathogen detection and microbiome data. It needs to pull in the following:\n",
    "    * sequence summary file from albacore\n",
    "    * two blast output files (specific database and NCBI)\n",
    "    * get the length of all porchoped reads\n",
    "    * get processing (T/F) for porchoped and nanolyze\n",
    "    * get this all into one data frame\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import subprocess as sub\n",
    "from Bio import SeqIO\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='''\n",
    "#This is a notebok that drafts a small script to pull in all the data for the WGS pathogen detection and microbiome data. It needs to pull in the following:\n",
    "\n",
    "#    * sequence summary file from albacore\n",
    "#    * two blast output files (specific database and NCBI)\n",
    "#    * get the length of all porchoped reads\n",
    "#    * get processing (T/F) for porchoped and nanolyze\n",
    "#    * get this all into one data frame\n",
    "#''')\n",
    "\n",
    "#parser.add_argument(\"BASEDIR\", help=\"base folder, supposed to have all the sub folders processed by WGS script. The same as Indir in YH_script2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets define the base folder\n",
    "BASEDIR = '/home/yiheng/data/Wagga_run2'\n",
    "#BASEDIR = args.BASEDIR\n",
    "#this will become the only flag of argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#write a quick check that looks for all the right folders\n",
    "folder_list = 'basecalled_data  scripts  tracking  workspace'.split(' ')\n",
    "for x in range(0,folder_list.count('')):\n",
    "    folder_list.remove('')\n",
    "#fix this test\n",
    "if not set(os.listdir(os.path.abspath(BASEDIR))) == set (folder_list):\n",
    "    print(\"Something wrong with basefolder. check it please.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the columns that you want to pick up from sequencing summary file.\n",
    "# Here is the columns I chose for plotting out data, enough information for me so I did not pick others.\n",
    "seq_df_headers = ['read_id','passes_filtering', 'sequence_length_template', 'mean_qscore_template',\\\n",
    "                  'barcode_arrangement', 'barcode_score', 'kit', 'variant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add a function to check the tar.gz file and its corresponding unzipped folder in tehe basecalled_data folder. \n",
    "# If it is not unzipped, unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now get the sequencing_summary file\n",
    "base_called_folder = os.path.join(BASEDIR, 'basecalled_data')\n",
    "for thing in os.listdir(base_called_folder):\n",
    "    if not os.path.isdir(os.path.join(base_called_folder, thing)):\n",
    "        next\n",
    "    else:\n",
    "        seq_sum_file = os.path.join(base_called_folder, thing, 'sequencing_summary.txt')\n",
    "        if not os.path.exists(seq_sum_file):\n",
    "            print('No sequencing summary file. Please go check')\n",
    "            continue\n",
    "        seq_df = pd.read_csv(seq_sum_file, sep='\\t')\n",
    "        #capture the thing as the prefix of the fastq/fasta files in the barcode folders\n",
    "        prefix = thing\n",
    "        #might be a better way to only read in the wanted columns. Not subsetting afterwards.\n",
    "        #please go check\n",
    "        seq_df = seq_df.loc[:, seq_df_headers].copy()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all barcode folders have all blast output files\n",
      "Not all barcode folders have all blast output files\n",
      "Not all barcode folders have all blast output files\n",
      "Not all barcode folders have all blast output files\n",
      "Not all barcode folders have all blast output files\n",
      "Not all barcode folders have all blast output files\n"
     ]
    }
   ],
   "source": [
    "#now get all the rgblast_output databases done \n",
    "rg_blast_df_file_list = []\n",
    "nt_blast_df_file_list = []\n",
    "workspace = os.path.join(BASEDIR, 'workspace')\n",
    "folder_counter = 0\n",
    "for folder in os.listdir(workspace):\n",
    "    folder = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    for file in os.listdir(folder):\n",
    "        if not file.endswith('.rgblast_output') or not file.endswith('.ntblast_output'):\n",
    "            next\n",
    "        if file.endswith('.rgblast_output'):\n",
    "            rg_blast_df_file_list.append(os.path.join(folder, file))\n",
    "        if file.endswith('.ntblast_output'):\n",
    "            nt_blast_df_file_list.append(os.path.join(folder, file))\n",
    "    if not len(nt_blast_df_file_list) == len(rg_blast_df_file_list) == folder_counter:\n",
    "        print('Not all barcode folders have all blast output files')\n",
    "    #print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode07/Wagga_run2_albacore202.barcode07.fastq\n",
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode08/Wagga_run2_albacore202.barcode08.fastq\n",
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode10/Wagga_run2_albacore202.barcode10.fastq\n",
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode09/Wagga_run2_albacore202.barcode09.fastq\n",
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode00/Wagga_run2_albacore202.barcode00.fastq\n",
      "now processing /home/yiheng/data/Wagga_run2/workspace/barcode11/Wagga_run2_albacore202.barcode11.fastq\n"
     ]
    }
   ],
   "source": [
    "#now get all the names of the reads that survived the lyzing\n",
    "workspace = os.path.join(BASEDIR, 'workspace')\n",
    "folder_counter = 0\n",
    "nl_readid_list = []\n",
    "for folder in os.listdir(workspace):\n",
    "    folder_long = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder_long):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    for file in os.listdir(folder_long):\n",
    "        lyzed_fastq = (os.path.join(folder_long, '%s.%s.fastq' % (prefix, folder )))\n",
    "        if not os.path.exists(lyzed_fastq):\n",
    "            print(\"No lyzed fastq file present.\")\n",
    "        elif file == '%s.%s.fastq' % (prefix, folder ):\n",
    "            print(\"now processing %s\" % lyzed_fastq)\n",
    "            lyzed_fastq_name = '%s.name.tmp' % lyzed_fastq\n",
    "            #cmd = \"grep '^@' %s | cut -c1-37 > %s\"%\\\n",
    "            # (lyzed_fastq, lyzed_fastq_name)\n",
    "            #cmd = \"grep '^@' %s | cut -d:-f1 > %s\"%\\\n",
    "             #(lyzed_fastq, lyzed_fastq_name)\n",
    "            cmd = r\"grep '^@' %s | sed -e 's/@\\([a-zA-Z0-9-]\\+\\).*/\\1/' > %s\"%\\\n",
    "            (lyzed_fastq, lyzed_fastq_name)\n",
    "            #print(cmd)\n",
    "            sub.check_output(cmd, shell=True, stderr=sub.STDOUT)\n",
    "            #sub.run(cmd.split(' '))\n",
    "            nl_readid_list.append(lyzed_fastq_name)\n",
    "        else:\n",
    "            next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now read in the tmp file of nanolyzed ids and add them to the dataframe as column\n",
    "def get_read_ids(filename_list):\n",
    "    #write a check that both _list and header is a list\n",
    "    read_id_list = []\n",
    "    for fn in filename_list:\n",
    "        tmp_list = pd.read_csv(fn, sep='\\t', header=None).loc[:,0].tolist()\n",
    "        read_id_list= read_id_list + tmp_list\n",
    "    return read_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nl_survied_list = get_read_ids(nl_readid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#do same for porechop + length\n",
    "#this is pretty slow. May consider parallzing.\n",
    "pc_length_dict = {}\n",
    "porechop_survived_list = []\n",
    "folder_counter = 0\n",
    "for folder in os.listdir(workspace):\n",
    "    folder_long = os.path.join(workspace,folder)\n",
    "    if not os.path.isdir(folder_long):\n",
    "        next\n",
    "    folder_counter += 1\n",
    "    porechoped_file = os.path.join(folder_long,'%s.chopped.%s.fastq'%(prefix, folder))\n",
    "    if not os.path.exists(porechoped_file):\n",
    "            print(\"Porechopped fastq missing for %s.\" % folder)\n",
    "    else:\n",
    "        for seq in SeqIO.parse(porechoped_file, 'fastq'):\n",
    "            pc_length_dict[seq.id] = len(seq.seq)\n",
    "            porechop_survived_list.append(seq.id)\n",
    "    \n",
    "#take for loop from above to loop over chopped files\n",
    "#filename = '/home/yiheng/data/Wagga_run1/workspace/barcode01/Wagga_run1_albacore202.chopped.barcode01.fastq'\n",
    "#for seq in SeqIO.parse(filename, 'fastq'):\n",
    "#    pc_length_dict[seq.id] = len(seq.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the porechopped reads will be splited into two reads they will have an _ in their read id\n",
    "porechop_survived_single_list = [x.split('_')[0] for x in porechop_survived_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add porechop survived column\n",
    "seq_df['pc_survived'] = seq_df['read_id'].isin(porechop_survived_single_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_id</th>\n",
       "      <th>passes_filtering</th>\n",
       "      <th>sequence_length_template</th>\n",
       "      <th>mean_qscore_template</th>\n",
       "      <th>barcode_arrangement</th>\n",
       "      <th>barcode_score</th>\n",
       "      <th>kit</th>\n",
       "      <th>variant</th>\n",
       "      <th>pc_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38b23f9f-2439-401d-8cd2-0fd5931d9a6c</td>\n",
       "      <td>True</td>\n",
       "      <td>469</td>\n",
       "      <td>11.009</td>\n",
       "      <td>barcode10</td>\n",
       "      <td>77.728813</td>\n",
       "      <td>BC</td>\n",
       "      <td>var2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b92829ad-a89d-4c16-83c1-073cd479a810</td>\n",
       "      <td>True</td>\n",
       "      <td>658</td>\n",
       "      <td>12.765</td>\n",
       "      <td>barcode11</td>\n",
       "      <td>81.084747</td>\n",
       "      <td>BC</td>\n",
       "      <td>var1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0620a179-f08f-442f-9b45-0667077f4e18</td>\n",
       "      <td>True</td>\n",
       "      <td>1021</td>\n",
       "      <td>12.231</td>\n",
       "      <td>barcode11</td>\n",
       "      <td>85.593224</td>\n",
       "      <td>BC</td>\n",
       "      <td>var2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2a0841c4-4f9c-4303-beea-b54ceb7af16d</td>\n",
       "      <td>True</td>\n",
       "      <td>609</td>\n",
       "      <td>8.068</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>48.711864</td>\n",
       "      <td>BC</td>\n",
       "      <td>var1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30a246f3-856f-4d09-ab1b-ce34ba661f69</td>\n",
       "      <td>False</td>\n",
       "      <td>159</td>\n",
       "      <td>5.341</td>\n",
       "      <td>unclassified</td>\n",
       "      <td>34.400002</td>\n",
       "      <td>BC</td>\n",
       "      <td>var2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                read_id  passes_filtering  \\\n",
       "0  38b23f9f-2439-401d-8cd2-0fd5931d9a6c              True   \n",
       "1  b92829ad-a89d-4c16-83c1-073cd479a810              True   \n",
       "2  0620a179-f08f-442f-9b45-0667077f4e18              True   \n",
       "3  2a0841c4-4f9c-4303-beea-b54ceb7af16d              True   \n",
       "4  30a246f3-856f-4d09-ab1b-ce34ba661f69             False   \n",
       "\n",
       "   sequence_length_template  mean_qscore_template barcode_arrangement  \\\n",
       "0                       469                11.009           barcode10   \n",
       "1                       658                12.765           barcode11   \n",
       "2                      1021                12.231           barcode11   \n",
       "3                       609                 8.068        unclassified   \n",
       "4                       159                 5.341        unclassified   \n",
       "\n",
       "   barcode_score kit variant  pc_survived  \n",
       "0      77.728813  BC    var2        False  \n",
       "1      81.084747  BC    var1        False  \n",
       "2      85.593224  BC    var2        False  \n",
       "3      48.711864  BC    var1         True  \n",
       "4      34.400002  BC    var2         True  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_df.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add the nanolyze survived column\n",
    "seq_df['nl_survived'] = seq_df['read_id'].isin(nl_survied_list)\n",
    "#make porchoped survived column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blast_header = ['qseqid',\n",
    " 'sseqid',\n",
    " 'evalue',\n",
    " 'bitscore',\n",
    " 'length',\n",
    " 'pident',\n",
    " 'nident',\n",
    " 'sgi',\n",
    " 'sacc',\n",
    " 'staxids',\n",
    " 'scomnames',\n",
    "'sskingdoms']\n",
    "# original headers: qseqid sseqid evalue bitscore length pident nident sgi sacc staxids sscinames scomnames sskingdoms\n",
    "def make_all_blast_df(_list, header, chopped_len_dict):\n",
    "    df = pd.DataFrame()\n",
    "    #write a check that both _list and header is a list\n",
    "    for x in _list:\n",
    "        tmp_df = pd.read_csv(x, sep='\\t',header=None, names=header)\n",
    "        first_column = tmp_df.columns[0]\n",
    "        tmp_df['read_id'] = tmp_df[first_column].apply(lambda x: str(x).split('_')[0])\n",
    "        tmp_df['read_length_pc'] = tmp_df[first_column].apply(lambda x: chopped_len_dict[x])\n",
    "        df = pd.concat([df, tmp_df.iloc[:,[0,1,2,4,5,6,8,9,10,12,13]]])\n",
    "        \n",
    "    #now reduce the columns to what we want\n",
    "    \n",
    "    #print(first_column)\n",
    "    #df['read_id'] = df.iloc[:,0].str.split('_')[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nt_df = make_all_blast_df(nt_blast_df_file_list, [x +'_nt' for x in  blast_header], pc_length_dict)\n",
    "rg_df = make_all_blast_df(rg_blast_df_file_list, [x +'_rg' for x in  blast_header], pc_length_dict)\n",
    "#reduce column number of blast dataframe to what you want before you merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'read_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2392\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2394\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'read_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8c574d7f2608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#need to check how merge behaves when getting a doublcate of value in one df.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtmp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrg_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'read_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'read_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'read_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'read_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m     51\u001b[0m                          \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_on\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                          \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                          copy=copy, indicator=indicator)\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator)\u001b[0m\n\u001b[1;32m    556\u001b[0m         (self.left_join_keys,\n\u001b[1;32m    557\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m          self.join_names) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yiheng/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2395\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'read_id'"
     ]
    }
   ],
   "source": [
    "#need to take care of porchop split reads checkin if second last character of string is _\n",
    "#make a new column of the blast_df that has the initial read_id\n",
    "#need to check how merge behaves when getting a doublcate of value in one df.\n",
    "tmp_df = pd.merge(seq_df, rg_df,how='outer',left_on= 'read_id', right_on='read_id')\n",
    "final_df = pd.merge(tmp_df, nt_df,how='outer',left_on= 'read_id', right_on='read_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2d058b8339eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinal_df_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASEDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'workspace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary_df.tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_df_fn = os.path.join(BASEDIR, 'workspace', 'summary_df.tab')\n",
    "final_df.to_csv(final_df_fn, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df.iloc[:200,0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_column(x):\n",
    "    if x in pc_length_dict.keys():\n",
    "        return pc_length_dict[x]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#porchop length column once pc_length_dict is done\n",
    "seq_df['pc_length'] = seq_df.read_id.apply(lambda x: length_column(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_df['pc_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove tmp files again\n",
    "pc_length_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to take care of porchop split reads checkin if second last character of string is _\n",
    "#make a new column of the blast_df that has the initial read_id\n",
    "#need to check how merge behaves when getting a doublcate of value in one df.\n",
    "test_df = pd.merge(seq_df, rg_df,how='outer',left_on= 'read_id', right_on='read_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_2 = test_df[test_df['qseqid_rg'].str.contains('_') == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.iloc[[992299,992300],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_2.loc[:,['read_id', 'qseqid_rg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df[test_df.read_id=='f7e483ca-c6b8-46be-975f-3bc433b3b0f6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.loc[:, ['read_id', 'qseqid_rg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now get the sequencing_summary file\n",
    "base_called_folder = os.path.join(BASEDIR, 'basecalled_data')\n",
    "for thing in os.listdir(base_called_folder):\n",
    "    if not os.path.isdir(os.path.join(base_called_folder, thing)):\n",
    "        next\n",
    "    else:\n",
    "        seq_sum_file = os.path.join(base_called_folder, thing, 'sequencing_summary.txt')\n",
    "        if not os.path.exists(seq_sum_file):\n",
    "            print('No sequencing summary file. Please go check')\n",
    "        seq_df = pd.read_csv(seq_sum_file, sep='\\t')\n",
    "        #might be a better way to only read in the wanted columns. Not subsetting afterwards.\n",
    "        #please go check\n",
    "        seq_df = seq_df.loc[:, seq_df_headers].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
